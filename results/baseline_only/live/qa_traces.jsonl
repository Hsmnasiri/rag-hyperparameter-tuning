{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 1, "total_items": 100, "question": "What did the Works Council Directive require?", "ground_truth": "workforce consultation in businesses", "all_answers": ["workforce consultation in businesses", "workforce consultation in businesses", "workforce consultation in businesses"], "prompt": "Answer the following question:\n\nQuestion: What did the Works Council Directive require?\n\nAnswer:", "prediction": "a Directive on the protection of the environment", "f1": 0.0, "semantic_similarity": 0.5217210464179516, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.15651631392538548}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 2, "total_items": 100, "question": "When did the Siege of Antioch take place?", "ground_truth": "1097", "all_answers": ["1097", "1097", "1097"], "prompt": "Answer the following question:\n\nQuestion: When did the Siege of Antioch take place?\n\nAnswer:", "prediction": "1912", "f1": 0.0, "semantic_similarity": 0.6193532943725586, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18580598831176756}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 3, "total_items": 100, "question": " In the 2009 Commission v Italy, case, the Court of Justice held that an Italian low prohibiting what infringed article 34?", "ground_truth": "motorcycles or mopeds pulling trailers", "all_answers": ["motorcycles or mopeds pulling trailers", "motorcycles or mopeds pulling trailers", "motorcycles or mopeds pulling trailers"], "prompt": "Answer the following question:\n\nQuestion:  In the 2009 Commission v Italy, case, the Court of Justice held that an Italian low prohibiting what infringed article 34?\n\nAnswer:", "prediction": "unanswerable", "f1": 0.0, "semantic_similarity": 0.5339458361268044, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1601837508380413}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 4, "total_items": 100, "question": "What can orthogonal forces be when there are three components with two at right angles to each other?", "ground_truth": "three-dimensional", "all_answers": ["three-dimensional", "three-dimensional", "three-dimensional", "three-dimensional"], "prompt": "Answer the following question:\n\nQuestion: What can orthogonal forces be when there are three components with two at right angles to each other?\n\nAnswer:", "prediction": "orthogonal forces", "f1": 0.0, "semantic_similarity": 0.6480861604213715, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19442584812641142}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 5, "total_items": 100, "question": "For how long did the plague stick around?", "ground_truth": "until the 19th century", "all_answers": ["until the 19th century", "until the 19th century", "19th century"], "prompt": "Answer the following question:\n\nQuestion: For how long did the plague stick around?\n\nAnswer:", "prediction": "a week", "f1": 0.0, "semantic_similarity": 0.6063224226236343, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1818967267870903}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 6, "total_items": 100, "question": "What separates the neuroimmune system and peripheral immune system in humans?", "ground_truth": "blood–brain barrier, blood–cerebrospinal fluid barrier", "all_answers": ["blood–brain barrier, blood–cerebrospinal fluid barrier", "blood–brain barrier, blood–cerebrospinal fluid barrier, and similar fluid–brain barriers", "blood–brain barrier, blood–cerebrospinal fluid barrier, and similar fluid–brain barriers", "fluid–brain barriers"], "prompt": "Answer the following question:\n\nQuestion: What separates the neuroimmune system and peripheral immune system in humans?\n\nAnswer:", "prediction": "immune system", "f1": 0.0, "semantic_similarity": 0.6257676482200623, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18773029446601866}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 7, "total_items": 100, "question": "About many students attend Kunskapsskolan schools?", "ground_truth": "10,000", "all_answers": ["10,000", "10,000", "nearly 10,000"], "prompt": "Answer the following question:\n\nQuestion: About many students attend Kunskapsskolan schools?\n\nAnswer:", "prediction": "more than 1 million", "f1": 0.0, "semantic_similarity": 0.8140606880187988, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.24421820640563963}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 8, "total_items": 100, "question": "Who was Emma's brother?", "ground_truth": "Duke Richard II", "all_answers": ["Duke Richard II", "Duke Richard II", "Duke Richard II"], "prompt": "Answer the following question:\n\nQuestion: Who was Emma's brother?\n\nAnswer:", "prediction": "Emma's brother", "f1": 0.0, "semantic_similarity": 0.6032759770751, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18098279312253}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 9, "total_items": 100, "question": "During what time period did income inequality decrease in the United States?", "ground_truth": "1910 to 1940", "all_answers": ["1910 to 1940", "1910 to 1940", "1910 to 1940"], "prompt": "Answer the following question:\n\nQuestion: During what time period did income inequality decrease in the United States?\n\nAnswer:", "prediction": "1950s", "f1": 0.0, "semantic_similarity": 0.7817140817642212, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.23451422452926635}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 10, "total_items": 100, "question": "How many societal class divisions were in the plan Kublai rejected?", "ground_truth": "three, later four", "all_answers": ["three, later four", "four", "three"], "prompt": "Answer the following question:\n\nQuestion: How many societal class divisions were in the plan Kublai rejected?\n\nAnswer:", "prediction": "2", "f1": 0.0, "semantic_similarity": 0.7583054900169373, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.22749164700508118}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 11, "total_items": 100, "question": "Who killed Harold II? ", "ground_truth": "William II", "all_answers": ["William II", "Duke William II", "Duke William II"], "prompt": "Answer the following question:\n\nQuestion: Who killed Harold II? \n\nAnswer:", "prediction": "Harold II", "f1": 0.5, "semantic_similarity": 0.811815619468689, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.4935446858406067}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 12, "total_items": 100, "question": "What is the name of the Egyptian papyrus that suggests that they may have had knowledge of prime numbers?", "ground_truth": "the Rhind papyrus", "all_answers": ["the Rhind papyrus", "Rhind", "Rhind", "Egyptian fraction", "Rhind papyrus"], "prompt": "Answer the following question:\n\nQuestion: What is the name of the Egyptian papyrus that suggests that they may have had knowledge of prime numbers?\n\nAnswer:", "prediction": "egypt", "f1": 0.0, "semantic_similarity": 0.6594952046871185, "exact_match": 0.0, "substring_match": 1.0, "fitness": 0.2978485614061356}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 13, "total_items": 100, "question": "What did the 18th century Huguenot group call themselves?", "ground_truth": "Camisards", "all_answers": ["Camisards", "Camisards", "the Camisards"], "prompt": "Answer the following question:\n\nQuestion: What did the 18th century Huguenot group call themselves?\n\nAnswer:", "prediction": "Huguenots", "f1": 0.0, "semantic_similarity": 0.6449063122272491, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19347189366817474}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 14, "total_items": 100, "question": "What helped spread Protestantism in France?", "ground_truth": "availability of the Bible in vernacular languages", "all_answers": ["availability of the Bible in vernacular languages", "the Bible in vernacular languages", "The availability of the Bible in vernacular languages"], "prompt": "Answer the following question:\n\nQuestion: What helped spread Protestantism in France?\n\nAnswer:", "prediction": "Protestantism", "f1": 0.0, "semantic_similarity": 0.6161452308297157, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1848435692489147}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 15, "total_items": 100, "question": "How did the plague infiltrate Alexandria?", "ground_truth": "y through the port's trade with Constantinople, and ports on the Black Sea", "all_answers": ["y through the port's trade with Constantinople, and ports on the Black Sea", "port's trade", "trade with Constantinople"], "prompt": "Answer the following question:\n\nQuestion: How did the plague infiltrate Alexandria?\n\nAnswer:", "prediction": "The plague spread to Alexandria, where it spread to the north.", "f1": 0.08, "semantic_similarity": 0.6244221478700638, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.22732664436101913}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 16, "total_items": 100, "question": "In a market economy, what is inequality a reflection of?", "ground_truth": "productivity gap between highly-paid professions and lower-paid professions", "all_answers": ["productivity gap between highly-paid professions and lower-paid professions", "productivity gap", "productivity gap"], "prompt": "Answer the following question:\n\nQuestion: In a market economy, what is inequality a reflection of?\n\nAnswer:", "prediction": "inequality", "f1": 0.0, "semantic_similarity": 0.6107573360204697, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18322720080614088}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 17, "total_items": 100, "question": "Where is the oldest known rock in the world located?", "ground_truth": "Slave craton in northwestern Canada", "all_answers": ["Slave craton in northwestern Canada", "Canada", "northwestern Canada"], "prompt": "Answer the following question:\n\nQuestion: Where is the oldest known rock in the world located?\n\nAnswer:", "prediction": "North America", "f1": 0.0, "semantic_similarity": 0.6792667508125305, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.20378002524375916}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 18, "total_items": 100, "question": "What article of the Grundgesetz grants the right to make private schools?", "ground_truth": "7", "all_answers": ["7", "7", "7"], "prompt": "Answer the following question:\n\nQuestion: What article of the Grundgesetz grants the right to make private schools?\n\nAnswer:", "prediction": "The Grundgesetz", "f1": 0.0, "semantic_similarity": 0.5804515331983566, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.174135459959507}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 19, "total_items": 100, "question": "What was the significance of victory at Forth Niagara for British?", "ground_truth": "cut off the French frontier forts further to the west and south", "all_answers": ["cut off the French frontier forts further to the west and south", "successfully cut off the French frontier forts further to the west and south", "cut off the French frontier forts", "cut off the French frontier forts", "successfully cut off the French frontier forts"], "prompt": "Answer the following question:\n\nQuestion: What was the significance of victory at Forth Niagara for British?\n\nAnswer:", "prediction": "victory at Forth Niagara for British", "f1": 0.0, "semantic_similarity": 0.5630710795521736, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1689213238656521}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 20, "total_items": 100, "question": "What do most online pharmacies do?", "ground_truth": "sell prescription drugs and require a valid prescription", "all_answers": ["sell prescription drugs and require a valid prescription", "sell prescription drugs", "sell prescription drugs and require a valid prescription"], "prompt": "Answer the following question:\n\nQuestion: What do most online pharmacies do?\n\nAnswer:", "prediction": "Most online pharmacies offer a wide variety of products.", "f1": 0.11764705882352941, "semantic_similarity": 0.7532065212726593, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.2847854857935625}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 21, "total_items": 100, "question": "What is the strongest main interaction?", "ground_truth": "strong", "all_answers": ["strong", "strong", "strong,", "strong", "strong", "strong, electromagnetic"], "prompt": "Answer the following question:\n\nQuestion: What is the strongest main interaction?\n\nAnswer:", "prediction": "interaction between two objects", "f1": 0.0, "semantic_similarity": 0.6004186049103737, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1801255814731121}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 22, "total_items": 100, "question": "Which TEFU article states that no quantitative restrictions can be placed on trade?", "ground_truth": "article 30", "all_answers": ["article 30", "TFEU article 30", "TFEU article 30"], "prompt": "Answer the following question:\n\nQuestion: Which TEFU article states that no quantitative restrictions can be placed on trade?\n\nAnswer:", "prediction": "TEFU", "f1": 0.0, "semantic_similarity": 0.5340086668729782, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.16020260006189346}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 23, "total_items": 100, "question": "When was Otto von Bismarck born?", "ground_truth": "1862", "all_answers": ["1862", "1862", "1862", "1862", "1862"], "prompt": "Answer the following question:\n\nQuestion: When was Otto von Bismarck born?\n\nAnswer:", "prediction": "19 January 1903", "f1": 0.0, "semantic_similarity": 0.7186476439237595, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.21559429317712783}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 24, "total_items": 100, "question": "Rather than taxation, what are private schools largely funded by?", "ground_truth": "tuition", "all_answers": ["tuition", "charging their students tuition", "tuition"], "prompt": "Answer the following question:\n\nQuestion: Rather than taxation, what are private schools largely funded by?\n\nAnswer:", "prediction": "taxation", "f1": 0.0, "semantic_similarity": 0.6469988226890564, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1940996468067169}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 25, "total_items": 100, "question": "The spin of what can produce a magnetic effect to oxygen molecules?", "ground_truth": "unpaired electrons", "all_answers": ["unpaired electrons", "unpaired electrons", "unpaired electrons", "unpaired electrons in the molecule", "unpaired electrons"], "prompt": "Answer the following question:\n\nQuestion: The spin of what can produce a magnetic effect to oxygen molecules?\n\nAnswer:", "prediction": "helium", "f1": 0.0, "semantic_similarity": 0.6143306195735931, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18429918587207794}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 26, "total_items": 100, "question": " J. A. Hobson wanted which races to develop the world?", "ground_truth": "highest 'social efficiency'", "all_answers": ["highest 'social efficiency'", "highest 'social efficiency'", "the races of highest 'social efficiency'\"", "of highest 'social efficiency", "races of highest 'social efficiency'\""], "prompt": "Answer the following question:\n\nQuestion:  J. A. Hobson wanted which races to develop the world?\n\nAnswer:", "prediction": "sydney", "f1": 0.0, "semantic_similarity": 0.5512914434075356, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.16538743302226067}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 27, "total_items": 100, "question": "What are the top 4-5% graduating students honored with?", "ground_truth": "summa cum laude", "all_answers": ["summa cum laude", "summa cum laude", "summa cum laude"], "prompt": "Answer the following question:\n\nQuestion: What are the top 4-5% graduating students honored with?\n\nAnswer:", "prediction": "honorary degrees", "f1": 0.0, "semantic_similarity": 0.6936689615249634, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.208100688457489}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 28, "total_items": 100, "question": "What time did a river system develop in the Upper Rhine Graben?", "ground_truth": "Miocene", "all_answers": ["Miocene", "By the time of the Miocene", "time of the Miocene"], "prompt": "Answer the following question:\n\nQuestion: What time did a river system develop in the Upper Rhine Graben?\n\nAnswer:", "prediction": "1300 to 1700", "f1": 0.0, "semantic_similarity": 0.5858760327100754, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1757628098130226}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 29, "total_items": 100, "question": "What entity deals with EU staff issues?", "ground_truth": "Civil Service Tribunal", "all_answers": ["Civil Service Tribunal", "Civil Service Tribunal", "Civil Service Tribunal", "Civil Service Tribunal"], "prompt": "Answer the following question:\n\nQuestion: What entity deals with EU staff issues?\n\nAnswer:", "prediction": "European Union", "f1": 0.0, "semantic_similarity": 0.626081570982933, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18782447129487992}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 30, "total_items": 100, "question": "Which outbreak was associated with troops in the thirty years war?", "ground_truth": "the Italian Plague of 1629–1631", "all_answers": ["the Italian Plague of 1629–1631", "Italian Plague", "Italian Plague"], "prompt": "Answer the following question:\n\nQuestion: Which outbreak was associated with troops in the thirty years war?\n\nAnswer:", "prediction": "apocalypse", "f1": 0.0, "semantic_similarity": 0.5521935224533081, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.16565805673599243}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 31, "total_items": 100, "question": "What is heralded by the sounding of the division bell?", "ground_truth": "\"Decision Time\"", "all_answers": ["\"Decision Time\"", "Decision Time", "\"Decision Time\""], "prompt": "Answer the following question:\n\nQuestion: What is heralded by the sounding of the division bell?\n\nAnswer:", "prediction": "syringe", "f1": 0.0, "semantic_similarity": 0.5383188761770725, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.16149566285312175}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 32, "total_items": 100, "question": "Why does Fresno only have UHF television stations?", "ground_truth": "To avoid interference with existing VHF television stations", "all_answers": ["To avoid interference with existing VHF television stations", "To avoid interference with existing VHF television stations", "To avoid interference with existing VHF television stations"], "prompt": "Answer the following question:\n\nQuestion: Why does Fresno only have UHF television stations?\n\nAnswer:", "prediction": "Because the UHF is not a radio station.", "f1": 0.0, "semantic_similarity": 0.7663512229919434, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.229905366897583}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 33, "total_items": 100, "question": "What is a regulatory factor produced by macrophages?", "ground_truth": "interleukin 1", "all_answers": ["interleukin 1", "interleukin 1", "interleukin 1"], "prompt": "Answer the following question:\n\nQuestion: What is a regulatory factor produced by macrophages?\n\nAnswer:", "prediction": "macrophages", "f1": 0.0, "semantic_similarity": 0.6715777516365051, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.20147332549095154}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 34, "total_items": 100, "question": "What decreased in number between 1984 and 1991?", "ground_truth": "quality rental units", "all_answers": ["quality rental units", "quality rental units", "quality rental units"], "prompt": "Answer the following question:\n\nQuestion: What decreased in number between 1984 and 1991?\n\nAnswer:", "prediction": "unanswerable", "f1": 0.0, "semantic_similarity": 0.5020467969588935, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.15061403908766804}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 35, "total_items": 100, "question": "What are malum prohibitum considerations?", "ground_truth": "things that are a matter of custom or expectation", "all_answers": ["things that are a matter of custom or expectation", "things that are a matter of custom or expectation", "things that are a matter of custom or expectation"], "prompt": "Answer the following question:\n\nQuestion: What are malum prohibitum considerations?\n\nAnswer:", "prediction": "Consideration of a fungus", "f1": 0.30769230769230765, "semantic_similarity": 0.5817821174860001, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.32838078909195384}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 36, "total_items": 100, "question": "What type of heating element is often used in toy steam engines?", "ground_truth": "electric", "all_answers": ["electric", "electric heating element", "electric"], "prompt": "Answer the following question:\n\nQuestion: What type of heating element is often used in toy steam engines?\n\nAnswer:", "prediction": "insulator", "f1": 0.0, "semantic_similarity": 0.7256715297698975, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.21770145893096923}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 37, "total_items": 100, "question": "Who did the Parliament rent additional buildings from?", "ground_truth": "City of Edinburgh Council", "all_answers": ["City of Edinburgh Council", "City of Edinburgh Council", "the City of Edinburgh Council"], "prompt": "Answer the following question:\n\nQuestion: Who did the Parliament rent additional buildings from?\n\nAnswer:", "prediction": "Parliament of England", "f1": 0.28571428571428575, "semantic_similarity": 0.666190505027771, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.3427142943654742}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 38, "total_items": 100, "question": "What is the primary goal of pleading not guilty when arrested for Civil Disobedience?", "ground_truth": "acquittal and avoid imprisonment", "all_answers": ["acquittal and avoid imprisonment", "win an acquittal and avoid imprisonment or a fine", "to use the proceedings as a forum", "win an acquittal", "to win an acquittal and avoid imprisonment or a fine"], "prompt": "Answer the following question:\n\nQuestion: What is the primary goal of pleading not guilty when arrested for Civil Disobedience?\n\nAnswer:", "prediction": "a conviction for a crime", "f1": 0.16666666666666666, "semantic_similarity": 0.7584429085254669, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.3108662058909734}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 39, "total_items": 100, "question": "Some have generalized the meaning of the word imperialism down to general-purpose what?", "ground_truth": "aggressiveness", "all_answers": ["aggressiveness", "aggressiveness", "aggressiveness", "aggressiveness", "aggressiveness"], "prompt": "Answer the following question:\n\nQuestion: Some have generalized the meaning of the word imperialism down to general-purpose what?\n\nAnswer:", "prediction": "general-purpose", "f1": 0.0, "semantic_similarity": 0.5893241316080093, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1767972394824028}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 40, "total_items": 100, "question": "What is the English translation of tawhid?", "ground_truth": "unity of God", "all_answers": ["unity of God", "unity of God", "unity of God"], "prompt": "Answer the following question:\n\nQuestion: What is the English translation of tawhid?\n\nAnswer:", "prediction": "tawhid", "f1": 0.0, "semantic_similarity": 0.5633540898561478, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.16900622695684434}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 41, "total_items": 100, "question": "Where was Marin's second fort constructed?", "ground_truth": "Fort Le Boeuf (present-day Waterford, Pennsylvania", "all_answers": ["Fort Le Boeuf (present-day Waterford, Pennsylvania", "Fort Le Boeuf", "present-day Waterford, Pennsylvania", "Fort Le Boeuf", "present-day Waterford, Pennsylvania"], "prompt": "Answer the following question:\n\nQuestion: Where was Marin's second fort constructed?\n\nAnswer:", "prediction": "Marin's second fort", "f1": 0.28571428571428575, "semantic_similarity": 0.6896612197160721, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.3497555087719645}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 42, "total_items": 100, "question": "What does less education lead to when working?", "ground_truth": "lower wages", "all_answers": ["lower wages", "lower wages", "lower wages"], "prompt": "Answer the following question:\n\nQuestion: What does less education lead to when working?\n\nAnswer:", "prediction": "a lack of education", "f1": 0.0, "semantic_similarity": 0.6940690577030182, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.20822071731090544}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 43, "total_items": 100, "question": "What type of revolution did Maududi advocate?", "ground_truth": "gradual", "all_answers": ["gradual", "Islamic revolution", "gradual"], "prompt": "Answer the following question:\n\nQuestion: What type of revolution did Maududi advocate?\n\nAnswer:", "prediction": "Revolutionary Revolution", "f1": 0.5, "semantic_similarity": 0.6429280042648315, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.44287840127944944}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 44, "total_items": 100, "question": "Who did Dinwiddie order to address French in Virginia territory?", "ground_truth": "Major George Washington", "all_answers": ["Major George Washington", "George Washington", "Major George Washington", "George Washington", "Major George Washington"], "prompt": "Answer the following question:\n\nQuestion: Who did Dinwiddie order to address French in Virginia territory?\n\nAnswer:", "prediction": "France", "f1": 0.0, "semantic_similarity": 0.616985946893692, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1850957840681076}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 45, "total_items": 100, "question": "How many streetcars did the Fresno Traction Company operate in 1931?", "ground_truth": "47 streetcars", "all_answers": ["47 streetcars", "47", "47"], "prompt": "Answer the following question:\n\nQuestion: How many streetcars did the Fresno Traction Company operate in 1931?\n\nAnswer:", "prediction": "four", "f1": 0.0, "semantic_similarity": 0.6612497568130493, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1983749270439148}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 46, "total_items": 100, "question": "What compiles and reports on data about the size of design and construction companies?", "ground_truth": "ENR", "all_answers": ["ENR", "Engineering News-Record (ENR)", "Engineering News-Record (ENR)"], "prompt": "Answer the following question:\n\nQuestion: What compiles and reports on data about the size of design and construction companies?\n\nAnswer:", "prediction": "Statistical Reports", "f1": 0.0, "semantic_similarity": 0.5591812580823898, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.16775437742471694}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 47, "total_items": 100, "question": "What is the second level of territorial division in Poland?", "ground_truth": "counties or powiats", "all_answers": ["counties or powiats", "counties or powiats", "counties or powiats"], "prompt": "Answer the following question:\n\nQuestion: What is the second level of territorial division in Poland?\n\nAnswer:", "prediction": "wiata", "f1": 0.0, "semantic_similarity": 0.5690236315131187, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.17070708945393562}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 48, "total_items": 100, "question": "In the layered model of the Earth, the mantle has two layers below it. What are they? ", "ground_truth": "the outer core and inner core", "all_answers": ["the outer core and inner core", "outer core and inner core", "outer core and inner core", "outer core and inner core"], "prompt": "Answer the following question:\n\nQuestion: In the layered model of the Earth, the mantle has two layers below it. What are they? \n\nAnswer:", "prediction": "a mantle", "f1": 0.0, "semantic_similarity": 0.6969645619392395, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.20908936858177185}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 49, "total_items": 100, "question": "What is the name of the Latin translation of Dioscorides' book?", "ground_truth": "De Materia Medica", "all_answers": ["De Materia Medica", "De Materia Medica", "De Materia Medica"], "prompt": "Answer the following question:\n\nQuestion: What is the name of the Latin translation of Dioscorides' book?\n\nAnswer:", "prediction": "Dioscorides", "f1": 0.0, "semantic_similarity": 0.640969455242157, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1922908365726471}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 50, "total_items": 100, "question": "For what reason would someone avoid crimes while protesting?", "ground_truth": "avoiding attribution", "all_answers": ["avoiding attribution", "viewed as not being a civil disobedient", "it would undermine the law by encouraging general disobedience", "constitute civil disobedience"], "prompt": "Answer the following question:\n\nQuestion: For what reason would someone avoid crimes while protesting?\n\nAnswer:", "prediction": "if you are a protester, you should avoid a crime.", "f1": 0.11764705882352941, "semantic_similarity": 0.5975383371114731, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.23808503054520663}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 51, "total_items": 100, "question": "What is the main goal of criminal punishment of civil disobedients?", "ground_truth": "incapacitation", "all_answers": ["incapacitation", "achieving crime control via incapacitation and deterrence", "achieving crime control via incapacitation and deterrence", "achieving crime control", "achieving crime control"], "prompt": "Answer the following question:\n\nQuestion: What is the main goal of criminal punishment of civil disobedients?\n\nAnswer:", "prediction": "punishment for a crime", "f1": 0.28571428571428575, "semantic_similarity": 0.6393708735704422, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.33466840492827554}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 52, "total_items": 100, "question": "Who recorded \"Walking in Fresno?\"", "ground_truth": "Bob Gallion", "all_answers": ["Bob Gallion", "Bob Gallion", "Bob Gallion"], "prompt": "Answer the following question:\n\nQuestion: Who recorded \"Walking in Fresno?\"\n\nAnswer:", "prediction": "edward wilson", "f1": 0.0, "semantic_similarity": 0.6149171143770218, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18447513431310653}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 53, "total_items": 100, "question": "What language other than English has the Scottish Parliament had meetings in?", "ground_truth": "Gaelic", "all_answers": ["Gaelic", "Gaelic", "Scots, Gaelic, or any other language with the agreement of the Presiding Officer"], "prompt": "Answer the following question:\n\nQuestion: What language other than English has the Scottish Parliament had meetings in?\n\nAnswer:", "prediction": "English", "f1": 0.0, "semantic_similarity": 0.729117289185524, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.2187351867556572}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 54, "total_items": 100, "question": "Where is the Hyde Park Day School located?", "ground_truth": "the University of Chicago campus", "all_answers": ["the University of Chicago campus", "University of Chicago campus", "the University of Chicago campus"], "prompt": "Answer the following question:\n\nQuestion: Where is the Hyde Park Day School located?\n\nAnswer:", "prediction": "New York City", "f1": 0.0, "semantic_similarity": 0.6425365060567856, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19276095181703568}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 55, "total_items": 100, "question": "What was the Dutch leader's religious affiliation?", "ground_truth": "Calvinist", "all_answers": ["Calvinist", "Calvinist", "Walloon"], "prompt": "Answer the following question:\n\nQuestion: What was the Dutch leader's religious affiliation?\n\nAnswer:", "prediction": "Christian", "f1": 0.0, "semantic_similarity": 0.7424947619438171, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.22274842858314514}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 56, "total_items": 100, "question": "What is an important factor contributing to inequality for individuals?", "ground_truth": "access to education", "all_answers": ["access to education", "access to education", "access to education"], "prompt": "Answer the following question:\n\nQuestion: What is an important factor contributing to inequality for individuals?\n\nAnswer:", "prediction": "e.g., the number of people living in poverty", "f1": 0.0, "semantic_similarity": 0.5787804499268532, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.17363413497805594}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 57, "total_items": 100, "question": "Complement proteins bind to what kind of molecules on the surface of microbes in order to elicit an immune response?", "ground_truth": "carbohydrates", "all_answers": ["carbohydrates", "carbohydrates", "carbohydrates"], "prompt": "Answer the following question:\n\nQuestion: Complement proteins bind to what kind of molecules on the surface of microbes in order to elicit an immune response?\n\nAnswer:", "prediction": "proteins", "f1": 0.0, "semantic_similarity": 0.6639363765716553, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19918091297149657}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 58, "total_items": 100, "question": "What do coastal beroids not have that other ctenophora have?", "ground_truth": "tentacles", "all_answers": ["tentacles", "tentacles", "tentacles"], "prompt": "Answer the following question:\n\nQuestion: What do coastal beroids not have that other ctenophora have?\n\nAnswer:", "prediction": "ctenophora", "f1": 0.0, "semantic_similarity": 0.6629419922828674, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19888259768486022}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 59, "total_items": 100, "question": "What was the original name of California State University at Fresno?", "ground_truth": "Fresno Normal School", "all_answers": ["Fresno Normal School", "Fresno Normal School", "Fresno Normal School"], "prompt": "Answer the following question:\n\nQuestion: What was the original name of California State University at Fresno?\n\nAnswer:", "prediction": "California State University at Fresno", "f1": 0.25, "semantic_similarity": 0.8472634553909302, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.37917903661727903}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 60, "total_items": 100, "question": "What is a spark or heat to the progress of a fire?", "ground_truth": "ignition event", "all_answers": ["ignition event", "ignition event", "an ignition event", "ignition event", "an ignition event"], "prompt": "Answer the following question:\n\nQuestion: What is a spark or heat to the progress of a fire?\n\nAnswer:", "prediction": "a spark", "f1": 0.0, "semantic_similarity": 0.7556495368480682, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.22669486105442047}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 61, "total_items": 100, "question": "Which border does the Rhine flow from the south?", "ground_truth": "Swiss-Austrian border", "all_answers": ["Swiss-Austrian border", "Swiss-Austrian", "Swiss-Austrian border"], "prompt": "Answer the following question:\n\nQuestion: Which border does the Rhine flow from the south?\n\nAnswer:", "prediction": "northwestern border", "f1": 0.4, "semantic_similarity": 0.714852824807167, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.41445584744215014}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 62, "total_items": 100, "question": "Who was made rich and prosperous prior to World War 1", "ground_truth": "many imperial powers", "all_answers": ["many imperial powers", "imperial powers", "imperial powers", "imperial powers"], "prompt": "Answer the following question:\n\nQuestion: Who was made rich and prosperous prior to World War 1\n\nAnswer:", "prediction": "aristotle", "f1": 0.0, "semantic_similarity": 0.5568758994340897, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1670627698302269}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 63, "total_items": 100, "question": "What civilization did the pottery belong to?", "ground_truth": "Timucua", "all_answers": ["Timucua", "Timucua people", "Mocama"], "prompt": "Answer the following question:\n\nQuestion: What civilization did the pottery belong to?\n\nAnswer:", "prediction": "Saxony", "f1": 0.0, "semantic_similarity": 0.6108080744743347, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1832424223423004}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 64, "total_items": 100, "question": "Maududi believed that Islam needed what to be established?", "ground_truth": "an Islamic state", "all_answers": ["an Islamic state", "an Islamic state", "an Islamic state"], "prompt": "Answer the following question:\n\nQuestion: Maududi believed that Islam needed what to be established?\n\nAnswer:", "prediction": "islam", "f1": 0.0, "semantic_similarity": 0.8653943538665771, "exact_match": 0.0, "substring_match": 1.0, "fitness": 0.35961830615997314}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 65, "total_items": 100, "question": "What did the Polish citizens understand the subtext of John Paul II's words to be?", "ground_truth": "incentive for the democratic changes", "all_answers": ["incentive for the democratic changes", "incentive for the democratic changes", "democratic changes"], "prompt": "Answer the following question:\n\nQuestion: What did the Polish citizens understand the subtext of John Paul II's words to be?\n\nAnswer:", "prediction": "symbiosis", "f1": 0.0, "semantic_similarity": 0.5065898499451578, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.15197695498354732}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 66, "total_items": 100, "question": "Where do other tourist events happen in Victoria outside of Melbourne?", "ground_truth": "regional cities", "all_answers": ["regional cities", "in regional cities", "Phillip Island"], "prompt": "Answer the following question:\n\nQuestion: Where do other tourist events happen in Victoria outside of Melbourne?\n\nAnswer:", "prediction": "Victoria, Victoria", "f1": 0.0, "semantic_similarity": 0.6271820664405823, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18815461993217467}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 67, "total_items": 100, "question": "Cydippid are typically what shape?", "ground_truth": "more or less rounded", "all_answers": ["more or less rounded", "egg-shaped", "more or less rounded"], "prompt": "Answer the following question:\n\nQuestion: Cydippid are typically what shape?\n\nAnswer:", "prediction": "slender", "f1": 0.0, "semantic_similarity": 0.6393878608942032, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19181635826826096}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 68, "total_items": 100, "question": "What year did Iqbal return to Lahore?", "ground_truth": "1908", "all_answers": ["1908", "1908", "1908"], "prompt": "Answer the following question:\n\nQuestion: What year did Iqbal return to Lahore?\n\nAnswer:", "prediction": "1922", "f1": 0.0, "semantic_similarity": 0.8113994896411896, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.24341984689235685}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 69, "total_items": 100, "question": "What are the three major types of rock? ", "ground_truth": "igneous, sedimentary, and metamorphic", "all_answers": ["igneous, sedimentary, and metamorphic", "igneous, sedimentary, and metamorphic", "igneous, sedimentary, and metamorphic", "igneous, sedimentary, and metamorphic"], "prompt": "Answer the following question:\n\nQuestion: What are the three major types of rock? \n\nAnswer:", "prediction": "sedimentary rock", "f1": 0.3333333333333333, "semantic_similarity": 0.7981467843055725, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.4061107019583384}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 70, "total_items": 100, "question": "What was the total nominal GDP of Warsaw in 2010?", "ground_truth": "191.766 billion PLN", "all_answers": ["191.766 billion PLN", "191.766 billion PLN", "191.766 billion PLN"], "prompt": "Answer the following question:\n\nQuestion: What was the total nominal GDP of Warsaw in 2010?\n\nAnswer:", "prediction": "2.7 billion", "f1": 0.28571428571428575, "semantic_similarity": 0.8210170865058899, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.3891622688089098}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 71, "total_items": 100, "question": "What did 'Da Yuan Tong Zhi' mean?", "ground_truth": "\"the comprehensive institutions of the Great Yuan\"", "all_answers": ["\"the comprehensive institutions of the Great Yuan\"", "the comprehensive institutions of the Great Yuan", "the comprehensive institutions of the Great Yuan"], "prompt": "Answer the following question:\n\nQuestion: What did 'Da Yuan Tong Zhi' mean?\n\nAnswer:", "prediction": "chinese", "f1": 0.0, "semantic_similarity": 0.6564891934394836, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19694675803184508}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 72, "total_items": 100, "question": "What religion did the Yuan discourage, to support Buddhism?", "ground_truth": "Taoism", "all_answers": ["Taoism", "Taoism", "Taoism"], "prompt": "Answer the following question:\n\nQuestion: What religion did the Yuan discourage, to support Buddhism?\n\nAnswer:", "prediction": "Buddhism", "f1": 0.0, "semantic_similarity": 0.791761577129364, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.2375284731388092}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 73, "total_items": 100, "question": "Who is responsible for axiomatic complexity theory?", "ground_truth": "Manuel Blum", "all_answers": ["Manuel Blum", "Manuel Blum", "Manuel Blum"], "prompt": "Answer the following question:\n\nQuestion: Who is responsible for axiomatic complexity theory?\n\nAnswer:", "prediction": "a physicist", "f1": 0.0, "semantic_similarity": 0.6310233771800995, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18930701315402984}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 74, "total_items": 100, "question": "When did the Germanic tribes claim territory in north and west Europe?", "ground_truth": "middle period of classical antiquity", "all_answers": ["middle period of classical antiquity", "the middle period of classical antiquity", "the middle period of classical antiquity", "the middle period of classical antiquity", "middle period of classical antiquity"], "prompt": "Answer the following question:\n\nQuestion: When did the Germanic tribes claim territory in north and west Europe?\n\nAnswer:", "prediction": "19th century", "f1": 0.0, "semantic_similarity": 0.7031804919242859, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.21095414757728576}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 75, "total_items": 100, "question": "When did the world's population finally recover from the black death?", "ground_truth": "the 17th century", "all_answers": ["the 17th century", "17th century", "17th century"], "prompt": "Answer the following question:\n\nQuestion: When did the world's population finally recover from the black death?\n\nAnswer:", "prediction": "September 21, 2017", "f1": 0.0, "semantic_similarity": 0.6152839958667755, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18458519876003265}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 76, "total_items": 100, "question": "After the Peterloo massacre what poet wrote The Massacre of Anarchy?", "ground_truth": "Percy Shelley", "all_answers": ["Percy Shelley", "Percy Shelley", "Percy Shelley", "Percy Shelley", "Percy Shelley"], "prompt": "Answer the following question:\n\nQuestion: After the Peterloo massacre what poet wrote The Massacre of Anarchy?\n\nAnswer:", "prediction": "edward wilson", "f1": 0.0, "semantic_similarity": 0.6793301403522491, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.20379904210567473}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 77, "total_items": 100, "question": "Who was in charge of the papal army in the War of Barbastro?", "ground_truth": "William of Montreuil", "all_answers": ["William of Montreuil", "William of Montreuil", "William of Montreuil"], "prompt": "Answer the following question:\n\nQuestion: Who was in charge of the papal army in the War of Barbastro?\n\nAnswer:", "prediction": "aristotle", "f1": 0.0, "semantic_similarity": 0.6603509932756424, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19810529798269272}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 78, "total_items": 100, "question": "When was the Muslim Brotherhood founded?", "ground_truth": "1928", "all_answers": ["1928", "1928", "1928"], "prompt": "Answer the following question:\n\nQuestion: When was the Muslim Brotherhood founded?\n\nAnswer:", "prediction": "1912", "f1": 0.0, "semantic_similarity": 0.8197311758995056, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.24591935276985166}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 79, "total_items": 100, "question": "According to polynomial time reduction squaring can ultimately be logically reduced to what?", "ground_truth": "multiplication", "all_answers": ["multiplication", "multiplication", "multiplication"], "prompt": "Answer the following question:\n\nQuestion: According to polynomial time reduction squaring can ultimately be logically reduced to what?\n\nAnswer:", "prediction": "squaring", "f1": 0.0, "semantic_similarity": 0.7449359893798828, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.22348079681396485}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 80, "total_items": 100, "question": "Who founded McKinsey & Company?", "ground_truth": "James O. McKinsey", "all_answers": ["James O. McKinsey", "James O. McKinsey", "James O. McKinsey"], "prompt": "Answer the following question:\n\nQuestion: Who founded McKinsey & Company?\n\nAnswer:", "prediction": "john w. kennedy", "f1": 0.0, "semantic_similarity": 0.737462654709816, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.22123879641294478}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 81, "total_items": 100, "question": "Where did Yesun Temur die?", "ground_truth": "Shangdu", "all_answers": ["Shangdu", "Shangdu", "Shangdu"], "prompt": "Answer the following question:\n\nQuestion: Where did Yesun Temur die?\n\nAnswer:", "prediction": "New York City", "f1": 0.0, "semantic_similarity": 0.5999038964509964, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1799711689352989}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 82, "total_items": 100, "question": "How many people were registered to receive the HD service prior to launch?", "ground_truth": "40,000", "all_answers": ["40,000", "40,000", "40,000"], "prompt": "Answer the following question:\n\nQuestion: How many people were registered to receive the HD service prior to launch?\n\nAnswer:", "prediction": "3 million", "f1": 0.0, "semantic_similarity": 0.8280550241470337, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.2484165072441101}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 83, "total_items": 100, "question": "What properties are analyzed with a conoscopic lens by petrologists? ", "ground_truth": "birefringence, pleochroism, twinning, and interference properties", "all_answers": ["birefringence, pleochroism, twinning, and interference properties", "birefringence, pleochroism, twinning, and interference", "birefringence, pleochroism, twinning, and interference"], "prompt": "Answer the following question:\n\nQuestion: What properties are analyzed with a conoscopic lens by petrologists? \n\nAnswer:", "prediction": "spectroscopy", "f1": 0.0, "semantic_similarity": 0.6694994568824768, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.20084983706474305}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 84, "total_items": 100, "question": "In which direction does most of the wind in Fresno originate from?", "ground_truth": "northwest", "all_answers": ["northwest", "northwest", "northwest"], "prompt": "Answer the following question:\n\nQuestion: In which direction does most of the wind in Fresno originate from?\n\nAnswer:", "prediction": "south-west", "f1": 0.0, "semantic_similarity": 0.8253660798072815, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.24760982394218445}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 85, "total_items": 100, "question": "After Braddock died, who controlled North American British forces?", "ground_truth": "William Shirley", "all_answers": ["William Shirley", "William Shirley", "William Shirley", "William Shirley", "William Shirley"], "prompt": "Answer the following question:\n\nQuestion: After Braddock died, who controlled North American British forces?\n\nAnswer:", "prediction": "allied forces", "f1": 0.0, "semantic_similarity": 0.5898206457495689, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.17694619372487067}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 86, "total_items": 100, "question": "What was the title of Philo's work?", "ground_truth": "Pneumatica", "all_answers": ["Pneumatica", "Pneumatica", "Pneumatica", "Pneumatica", "Pneumatica"], "prompt": "Answer the following question:\n\nQuestion: What was the title of Philo's work?\n\nAnswer:", "prediction": "", "f1": 0.0, "semantic_similarity": 0.0, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.0}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 87, "total_items": 100, "question": "What type of surveys show the location of stratigraphic units in the subsurface?", "ground_truth": "geophysical surveys", "all_answers": ["geophysical surveys", "geophysical", "geophysical"], "prompt": "Answer the following question:\n\nQuestion: What type of surveys show the location of stratigraphic units in the subsurface?\n\nAnswer:", "prediction": "surveys", "f1": 0.6666666666666666, "semantic_similarity": 0.7049719393253326, "exact_match": 0.0, "substring_match": 1.0, "fitness": 0.6448249151309331}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 88, "total_items": 100, "question": "The house of the Baryczko merchant family is a notable example of what type of architecture?", "ground_truth": "Renaissance", "all_answers": ["Renaissance", "Renaissance", "Renaissance"], "prompt": "Answer the following question:\n\nQuestion: The house of the Baryczko merchant family is a notable example of what type of architecture?\n\nAnswer:", "prediction": "architectural style", "f1": 0.0, "semantic_similarity": 0.646227702498436, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.1938683107495308}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 89, "total_items": 100, "question": "How many people in Quzhou are descended from Confucius?", "ground_truth": "30,000", "all_answers": ["30,000", "30,000", "30,000"], "prompt": "Answer the following question:\n\nQuestion: How many people in Quzhou are descended from Confucius?\n\nAnswer:", "prediction": "3000", "f1": 0.0, "semantic_similarity": 0.8724016547203064, "exact_match": 0.0, "substring_match": 1.0, "fitness": 0.36172049641609194}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 90, "total_items": 100, "question": "Aside from being located on the coast, what contributes to Jacksonville's lack of cold weather?", "ground_truth": "low latitude", "all_answers": ["low latitude", "low latitude", "low latitude"], "prompt": "Answer the following question:\n\nQuestion: Aside from being located on the coast, what contributes to Jacksonville's lack of cold weather?\n\nAnswer:", "prediction": "a hurricane", "f1": 0.0, "semantic_similarity": 0.6178059056401253, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.18534177169203758}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 91, "total_items": 100, "question": "Who decides who gets to speak in the chamber debates?", "ground_truth": "The Presiding Officer", "all_answers": ["The Presiding Officer", "Presiding Officer", "The Presiding Officer"], "prompt": "Answer the following question:\n\nQuestion: Who decides who gets to speak in the chamber debates?\n\nAnswer:", "prediction": "the Senate", "f1": 0.4, "semantic_similarity": 0.7275157570838928, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.41825472712516787}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 92, "total_items": 100, "question": "With what European country did the Huguenots feel kinship for emigration to?", "ground_truth": "Dutch Republic", "all_answers": ["Dutch Republic", "Dutch Republic", "Dutch Republic"], "prompt": "Answer the following question:\n\nQuestion: With what European country did the Huguenots feel kinship for emigration to?\n\nAnswer:", "prediction": "Hungary", "f1": 0.0, "semantic_similarity": 0.7831440567970276, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.23494321703910825}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 93, "total_items": 100, "question": "What kind of sending technology is being used to protect tribal lands in the Amazon?", "ground_truth": "remote sensing", "all_answers": ["remote sensing", "remote sensing", "remote sensing"], "prompt": "Answer the following question:\n\nQuestion: What kind of sending technology is being used to protect tribal lands in the Amazon?\n\nAnswer:", "prediction": "e-mail", "f1": 0.0, "semantic_similarity": 0.5730102509260178, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.17190307527780532}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 94, "total_items": 100, "question": "What issues were not addressed in the Treaty of Aix-la-Chapelle?", "ground_truth": "conflicting territorial claims between British and French", "all_answers": ["conflicting territorial claims between British and French", "conflicting territorial claims between British and French colonies in North America", "conflicting territorial claims between British and French colonies in North America", "conflicting territorial claims", "The issues of conflicting territorial claims between British and French colonies"], "prompt": "Answer the following question:\n\nQuestion: What issues were not addressed in the Treaty of Aix-la-Chapelle?\n\nAnswer:", "prediction": "The Treaty of Aix-la-Chapelle", "f1": 0.23529411764705885, "semantic_similarity": 0.6914209127426147, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.32507333264631383}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 95, "total_items": 100, "question": "When was gold found near Ballarat?", "ground_truth": "1851", "all_answers": ["1851", "in 1851", "1851"], "prompt": "Answer the following question:\n\nQuestion: When was gold found near Ballarat?\n\nAnswer:", "prediction": "1904", "f1": 0.0, "semantic_similarity": 0.722724437713623, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.2168173313140869}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 96, "total_items": 100, "question": "Warner Center is located in which area?", "ground_truth": "San Fernando Valley", "all_answers": ["San Fernando Valley", "the San Fernando Valley", "San Fernando Valley"], "prompt": "Answer the following question:\n\nQuestion: Warner Center is located in which area?\n\nAnswer:", "prediction": "south of the city", "f1": 0.25, "semantic_similarity": 0.7371198236942291, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.3461359471082687}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 97, "total_items": 100, "question": "What person helped establish the school with a donation?", "ground_truth": "John D. Rockefeller", "all_answers": ["John D. Rockefeller", "John D. Rockefeller", "John D. Rockefeller", "John D. Rockefeller"], "prompt": "Answer the following question:\n\nQuestion: What person helped establish the school with a donation?\n\nAnswer:", "prediction": "a philanthropist", "f1": 0.0, "semantic_similarity": 0.7089841067790985, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.21269523203372956}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 98, "total_items": 100, "question": "What flood impacted the Meuse?", "ground_truth": "St. Elizabeth's", "all_answers": ["St. Elizabeth's", "St. Elizabeth's", "St. Elizabeth's flood"], "prompt": "Answer the following question:\n\nQuestion: What flood impacted the Meuse?\n\nAnswer:", "prediction": "Hurricane Katrina", "f1": 0.0, "semantic_similarity": 0.5599415153264999, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.16798245459794997}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 99, "total_items": 100, "question": "What British mathematician took pride in doing work that he felt had no military benefit?", "ground_truth": "G. H. Hardy", "all_answers": ["G. H. Hardy", "G. H. Hardy", "G. H. Hardy", "G. H. Hardy"], "prompt": "Answer the following question:\n\nQuestion: What British mathematician took pride in doing work that he felt had no military benefit?\n\nAnswer:", "prediction": "edward wilson", "f1": 0.0, "semantic_similarity": 0.6443420201539993, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.19330260604619978}
{"kind": "baseline_no_rag_item", "trace": {"baseline": "no_rag_llm"}, "item": 100, "total_items": 100, "question": "What famous civil rights leader called Harvard home?", "ground_truth": "W. E. B. Du Bois", "all_answers": ["W. E. B. Du Bois", "W. E. B. Du Bois", "W. E. B. Du Bois"], "prompt": "Answer the following question:\n\nQuestion: What famous civil rights leader called Harvard home?\n\nAnswer:", "prediction": "george w. bush", "f1": 0.25, "semantic_similarity": 0.6303988397121429, "exact_match": 0.0, "substring_match": 0.0, "fitness": 0.3141196519136429}
